{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# nltk processing\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load the data\n",
    "review_docs = \n",
    "\n",
    "\n",
    "## have only the business_id and text columns\n",
    "review_docs = review_docs.filter(['business_id', 'text'], axis = 1)\n",
    "review_docs.groupby('business_id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean all the text\n",
    "stopwrds = stopwords.words('english')\n",
    "def cleaning_text(sentence):\n",
    "    sentence = str(sentence)   #sentence as string\n",
    "    sentence = sentence.lower()   #lowercase it all\n",
    "    sentence = re.sub('[^\\w\\s]',' ', sentence)    # replace whitespace with single space\n",
    "    sentence = re.sub('_',' ', sentence)    #substitute under with whitespace\n",
    "    sentence = re.sub('\\d+',' ', sentence)      #substitute numbers with whitespace\n",
    "    cleaned = ' '.join([w for w in sentence.split() if not w in stopwrds])     #split sentence into list by words and remove stopwords\n",
    "    cleaned = ' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])   #create sentece of word followed by its POS if its POS is of a certain type\n",
    "    cleaned = ' '.join([w for w in cleaned.split() if not len(w)<=2 ])    #remove words less than two chars long (which means the POS tags are removed)\n",
    "    cleaned = cleaned.strip()    #return list of words that are left and cleaned \n",
    "    return cleaned\n",
    "\n",
    "  \n",
    "# add utf-8 encoding\n",
    "review_docs['textClean'] = review_docs.apply(lambda row: cleaning_text(row['text'].encode(\"utf8\")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_sentence(sentence):\n",
    "    words = re.split('\\W+', sentence.lower())\n",
    "    return [word for word in words if word != \"\"]\n",
    "\n",
    "# MyDocs reading from a data frame\n",
    "class MyDocs(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(review_docs.shape[0]):\n",
    "            yield doc2vec.TaggedDocument(words=split_sentence(review_docs.iloc[i,1]), tags=['%s' % review_docs.iloc[i,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the doc2vec model\n",
    "mydocs = MyDocs()\n",
    "model = doc2vec.Doc2Vec(mydocs, size = 200, window = 8, min_count = 5, workers = 4)\n",
    "model.save(\"review.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test similar words  (I'm hoping this brings up \"bun\")\n",
    "print model.most_similar(positive = ['tortilla', 'american'], negative = ['mexican'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save a pickle file\n",
    "import pickle\n",
    "\n",
    "with open('models/doc2vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving docvecs\n",
    "review_docvecs = model.docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pickle file\n",
    "import pickle\n",
    "with open('review_docvecs.pickle', 'wb') as handle:\n",
    "    pickle.dump(review_docvecs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity/Recommender\n",
    "## ignore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between two vecotrs \n",
    "def cossim(v1, v2): \n",
    "    return np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / np.sqrt(np.dot(v2, v2)) \n",
    "\n",
    "# return top_n values from a list\n",
    "def top_n_index(l,n):\n",
    "    return sorted(range(len(l)), key=lambda i: l[i])[-(n+1):-1] #-1 to take off the own product from the returned index list\n",
    "\n",
    "\n",
    "def content_recommend(business_id, top_n, business_lookup, inputs):\n",
    "    input_vec = inputs['business_id' == business_id]\n",
    "    \n",
    "    #compute similarity array\n",
    "    sim_array = map(lambda v: cossim(input_vec, v), inputs)\n",
    "    \n",
    "    # recommendation's index (set to 50 to get enough to filter out later)\n",
    "    recommendation_index = top_n_index(sim_array, 500)\n",
    "    \n",
    "    # recommendation's unique id\n",
    "    recommendation_unique_id = [i+1 for i in recommendation_index]\n",
    "    \n",
    "    # recommendation's cossim values\n",
    "    recommendation_cossim = [sim_array[i] for i in recommendation_index]\n",
    "    \n",
    "    top_products = zip(recommendation_unique_id, recommendation_cossim)\n",
    "    \n",
    "    # get the range of unique id for a given category prefered by user\n",
    "    category_range = category_id_range(category_list)\n",
    "    \n",
    "    result = [i for i in top_products if i[0] in category_range]\n",
    "    \n",
    "    return result[-top_n:] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
